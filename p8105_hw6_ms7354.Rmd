---
title: "p8105_hw6"
author: "Maliha Safdar"
date: "2025-11-29"
output: github_document
---



```{r}
library(tidyverse)
library(modelr)
library(broom)
library(p8105.datasets)
weather_df = p8105.datasets::weather_df
set.seed(123)
```

### Problem 1

`First, we will go ahead and import the data.`
```{r}
### importing the dataset

homicide_daf = read.csv("homicide-data.csv")
```


`Next, create city_state and solved variable. The solved variable will be binary so any case that is not closed with arrest will be 0. The victim_age will be converted to numeric using mutate function. We will use the filter function to ensure that Dallas, Phoenix, Kansas City and Tulsa are removed from city_state variable as well as only including White and Black race in victim_race.` 

```{r}
### creating city_state variable, making binary variable for disposition, omitting Dallas, Phoenix, Kansas City and Tulsa,selecting white+black for victime_race and contverting victime_age to numeric.

homicide_df = homicide_daf |>
  mutate(
    city_state = str_c(city,state, sep = ","),
    solved = if_else(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
    ) |>
  filter(
    !city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO","Tulsa,AL"),
    victim_race %in% c ("White", "Black")
  )
```

`Now we will fit a logistic regression  with solved and unsolved homicide cases as outcome and victime age, sex and race as predictors. For this model we will use Baltimore, MD from the city_state variable.`

```{r}
#filter for Baltimore, MD and create a variable for it.

baltimore <- homicide_df |>
  filter(city_state%in% c("Baltimore,MD"))

# create the logistical model

log_model <- glm(solved ~ victim_age + victim_sex + victim_race, data = baltimore, family = binomial())

log_model |>
  tidy(conf.int = TRUE) |>
  filter(term == "victim_sexMale") |>
  mutate(
    OR = exp(estimate),
    CI_low = exp(conf.low),
    CI_high = exp(conf.high)
  ) |>
  select(OR, CI_low, CI_high) |>
  knitr::kable(digits = 3)
```

`When comparing males versus females, we see that the odds ratio for solving homicide cases is 0.426 with a confidence interval between 0.324 and 0.558.`


`Next we will perform logistic regression for every city in the dataset, and extract the odds ratio and CIs. We will compare males vs female victims.`

```{r, warning=FALSE, message=FALSE}

cities <- homicide_df |>
  group_by(city_state) |>
  nest() |>
  mutate(
    model = map(data, ~ glm(solved ~ victim_age + victim_sex + victim_race, data= .x, family = binomial ())),
    results = map(model, ~ broom::tidy(.x, conf.int = TRUE))
  ) |>
  unnest(results) |>
  filter(term == "victim_sexMale") |>
  mutate(
    OR = exp(estimate),
    CI_low = exp(conf.low),
    CI_high = exp(conf.high)
  ) |>
  select(city_state, OR, CI_low, CI_high)

cities |>
  knitr::kable(digits = 3)
```

`Making a plot to show the estimated ORs and CI for each city.`

```{r}

cities |>
  ggplot(aes(x = reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high)) +
  labs(
    title = "Est Odds Ration for Solved Homicide Cases For Each City",
    x = "City, State",
    y = " Odds ratio (Male vs Female)"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
```
`In the plot above, we can visualize the Odds Ratio of solved homicide cases in every city of our dataset. The ORs vary between 0.5 to 1.5. In cities like NYC, Baton Rouge and Omaha the OR is less than 1 which indicates that in those cities female homicide cases are solved more than males. In cities like Albuquerque, Stockon, and Fresno which have ORs greater than 1, indicates that male homicide cases are solved more than females.`

### Problem 2
`We will fit a simple linear regression model using weather_df. The tmax is the output(response) and tmin and prcp are predictors.`
```{r}
# Create the function
weather_bootstrap <- function(df) {
 
   bootstrap_df <- df |> sample_frac(replace = TRUE)
   
   fit <- lm(tmax ~ tmin + prcp, data = bootstrap_df)
   
   r2 <- glance(fit)$r.squared #extract r2
   
   coefs <- tidy(fit) #extract coeffcients
   
   beta1 <- coefs$estimate[coefs$term == "tmin"]
   beta2 <- coefs$estimate[coefs$term == "prcp"]
   
   tibble(
     r2 = r2,
     coef_values = beta1 * beta2
   )
}

# Taking bootstrap samples

weather_bootstrap_results <- map_df(1:5000, ~ weather_bootstrap(weather_df))
```


```{r}
weather_bootstrap_results |>
  ggplot(aes( x = r2)) +
  geom_density(fill = "red", alpha = 0.5) +
  labs(
    title = "Distribution of R-Squared from Bootstrap Samples"
  )
```

```{r}

weather_bootstrap_results |>
  ggplot(aes(x = coef_values)) +
  geom_density(fill = "lightblue", alpha = 0.8) +
  labs(
    title = "Distribution of Beta 1 * Beta 2"
  )
```

`Now we'll calculate the 95% confidence intervals for r-squared and Beta1*Beta2.`

```{r}
conf_intervals <- weather_bootstrap_results |>
  summarise(
    r2_low = quantile(r2, 0.025),
    r2_high = quantile(r2, 0.975),
    beta_low = quantile(coef_values, 0.025),
    beta_high = quantile(coef_values, 0.975)
  )

knitr::kable(conf_intervals)
```



### Problem 3

`First, we will import the dataset.`

```{r}
birthweight = read.csv("birthweight.csv")

```

`Now we will clean the dataset.`
```{r}
birthweight_clean <- birthweight |>
  mutate(
    babysex = 
      case_match(
        babysex, 
        1 ~ "male",
        2 ~ "female"
      ),
    babysex = fct_infreq(babysex),
    frace = 
      case_match(
        frace, 
        1 ~ "White",
        2 ~ "Black",
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other",
        9 ~ "Unknown"
      ),
    frace = fct_infreq(frace),
    mrace = 
      case_match(
        mrace, 
        1 ~ "White",
        2 ~ "Black",
        3 ~ "Asian",
        4 ~ "Puerto Rican",
        8 ~ "Other",
        9 ~ "Unknown"),
    mrace = fct_infreq(mrace),
    malform = as.logical(malform)) |>
  drop_na()
```

`Next we will make a regression model for birthweight.`

```{r}
birthweight_model <- lm( bwt ~ babysex + bhead + blength + delwt + fincome + frace+ gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight_clean)

birthweight_model
```
`Now we will plot the residuals against the fitted values.`

```{r}
birthweight_df <- birthweight_clean |>
  add_predictions(birthweight_model) |>
  add_residuals(birthweight_model)
  

birthweight_df |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth() +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values (Predicted Birthweight",
    y = "Residuals"
  )
```

`A baby's birth weight is affected by many variables. Some of the baby's characteristics such as sex, head circumference, and length at birth directly affect the growth and development while the mother's health also plays a role in the baby's birth weight. Some of those variables include pre-pregnancy BMI, weight, age and height. Furtermore, socioeconomic factors such as family income, mother and father's race are also predictors of birth weight. Lastly, certain lifestyle and health measurements like gestational age, smoking behavior and parity affect fetal growth and determine the baby's birthweight.`

`Now we will compare the model to two other models.`

```{r, models}

model_1 <- lm( bwt ~ babysex + bhead + blength + delwt + fincome + frace+ gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight_clean)

model_2 <- lm(bwt ~ blength + gaweeks, data = birthweight_clean)

model_3 <- lm(bwt ~ bhead * blength * babysex, data = birthweight_clean)
```


`Now we will show the comparisons using cross-validated prediction error.`
```{r}
cross_validation <- crossv_mc(birthweight_df, 100) |>
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cross_validation <- cross_validation |>
  mutate(
    model_1 = map(train, \(df) lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace+ gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = df)),
    model_2 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    model_3 = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df)),
    rmse_model_1 = map2_dbl(model_1, test, \(mod,df) rmse(model = mod, data = df)),
    rmse_model_2 = map2_dbl(model_2, test, \(mod,df) rmse(model = mod, data = df)),
    rmse_model_3 = map2_dbl(model_3, test, \(mod,df) rmse(model = mod, data = df))
    
  )
```

`Now we will plot the output of each model.`

```{r}
cross_validation |>
  select(starts_with("rmse")) |>
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") |>
  mutate(
    model = fct_inorder(model)) |>
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs( title = "Cross-validated prediction error for birth weight models")
```
`The above violin plots displays the three models that predict birth weight in babies. Model 1 uses all the variables that include the baby, mother and socioeconomic measurements which in turn gives the lowest RMSE which means that it makes accurate predictions on birth weight. Model 2 uses  only two variables, the baby's length and mother's gestational age which is why it has high RMSE values indicating that it does not make accurate predictions on baby's weight. Model 3 includes baby's features like the length, sex and head size which performs better and has lower RMSE values.`
